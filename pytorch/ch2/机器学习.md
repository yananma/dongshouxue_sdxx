
### 重要问题

### 期望的公式

#### 1、xgboost 相关，会问的很细，要读多遍论文
②、为什么要二阶展开  
③、xgboost采样的时候怎么采样的  
④、手写xgboost的目标函数，xgboost和GBDT的区别，xgboost构建树时候节点分裂的公式是什么？ xgboost如何调参,xgboost可以自定义损失函数吗？给定一个场景如何自定义损失函数？如果样本的权重不一样如何自定义损失函数？sklearn的xgboost支持哪些损失函数？分类和回归算法都有哪些损失函数？模型融合如何做的？bagging，boosting和stacking的原理以及他们的区别是是什么；

#### 2、LR 是问的最多的

为什么不用最小二乘  


#### 3、gbdt


#### 4、写出极大似然估计公式


#### 5、kmeans
手写 Kmeans 代码(debug 一遍，如果再看到，就)、如何初始化、聚的是特征还是样本？特征的距离如何计算？  
什么数据不能聚：圆环套圆圈、笑脸、，非凸数据  

已解决：
具体步骤：  
1、随机选择 k 个样本点作为初始聚类中心  
2、计算每个样本到类中心的距离，将每个样本指派到与其最近的中心的类中，得到一个聚类结果  
3、更新每个类的样本的均值，作为类的新的中心  
4、重复上面两个步骤，直到收敛为止  

k 值如何选择？P267 尝试不同的 k 值，选择最优的。  


#### 6、SVM： 拉格朗日乘子，KKT条件，对偶问题，核方法是什么，用过哪些核函数


#### 7、特征工程
怎么清洗数据、特征筛选，怎么找出相似性高的特征并去掉、如何比较feature importance、不平衡样本处理(六次看到)（迪哥课程里有）、特征数值范围很大怎么办、  


#### 知道的 loss 都有哪些？写一下公式，写一下 softmax 公式
L1、MSE、cross-entropy（讲一讲交叉熵，写出交叉熵公式 四次看到了。d2l）、

#### 谈谈对 PCA 的认识  
PCA的流程，PCA第一步为什么要中心化、

#### 介绍 GBDT  

决策树如何剪枝的





1、AUC值，PR曲线，ROC曲线；pr曲线，roc曲线。L2正则，L1正则

2、交叉熵和最大似然估计的关系推导

3、Kmeans原理和实现

4、GDBT、Xgboost、Lightgbm原理和区别

5、Dropout在训练和测试时的区别

6、随机森林，随机性体现在哪里

7、Transformer的实现细节

8、CRF和HMM

9、Word2vec Hierarchical Softmax原理

10、BERT模型架构，怎么预训练，怎么feature based/fine-tune

12、LSTM 和 GRU 门机制

13、GBDT原理。追问：负梯度的概念是什么，为什么用负梯度

14、GBDT 叶节点权重怎么算

15、GBDT 单调性约束怎么实现的

16、能否用LR 或者 SVM 做GBDT基模型

17、RF 原理讲一下，为什么能防止过拟合

18、机器学习如何调参

19、Dropout实现上的细节, 代码实现

20、BN细节，Dropout和BN之间的区别

21、Bert最新的发展，都是为了解决什么问题

22、Attention原理以及实现Attention

23、cnn模型并行和数据并行有什么区别

24、softmax公式及求导

25、交叉熵，相对熵，为什么损失函数用交叉熵

26、10亿个数，内存只有1M，如何让这10亿个数有序

27、模型训练为什么要引入偏差和方差

28、如何解决过拟合的问题

29、朴素贝叶斯为什么朴素naïve

30、One-hot的作用为什么不直接用数字表示

31、LR明明是分类模型为什么叫回归

32、梯度下降如何并行化

33、LR中L1/L2正则项是啥，哪个好，为什么L1正则稀疏性

34、写出全概率公式&贝叶斯公司并对比

35、SVM推导以及如何解决多分类问题

36、分层分类在实际问题中如何解决

37、决策树和随机森林的区别是什么

38、Kmeans初始点除了随机选择之外的方法


### 整理完看牛客网的收藏  

### 一般小问题  

1、SVM 损失函数  
2、为什么AUC面积大证明性能强  
3、
4、



### 已经解决的问题

#### LR 
LR 公式抄了 6 遍了、LR 损失函数就是 L(w)  

#### 解决过拟合的方法
增加数据、正则化、Dropout、early stopping、batch norm、  

#### 为什么 L2 可以解决过拟合
权重衰减通过惩罚绝对值较大的参数，为需要学习的模型增加了限制（可以看自己的笔记 d2l 66 页）  

#### 了解什么评估指标
交叉验证是模型评估方法、recall、precision、F1、AUC、介绍一下 ROC 曲线（西瓜书）、P、R和AP有什么关系？、  

#### DBSCAN
传染病一样，在范围内，发展下线 要指定半径、  
优点：不需要制定簇的个数、可以发现任意形状的簇、  
缺点：参数难以选择，参数对结果影响非常大、训练很慢、


Naive Bayes, random forest, svm, logistic regression, xgboost, gdbt全家桶，每个都详细问讲区别讲联系讲损失函数
一些模型独有的问题：
- svm核函数 种类，如何选择，如何应用
- random forest引出 bagging boosting，区别
- random forest 分割依据 （忘了）
- random forest 随机在哪里（特征，样本选择）
- lr 和 rf 在输入上的不同
- xgboost 二阶导数
- 等

HMM CRF双雄 Viterbi

tf-idf 公式 如何用tf-idf计算文本中的关键词




