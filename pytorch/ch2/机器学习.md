
### 重要问题

#### 1、xgboost 相关
①、
②、为什么要二阶展开  
③、xgboost采样的时候怎么采样的  
④、手写xgboost的目标函数，xgboost和GBDT的区别，xgboost构建树时候节点分裂的公式是什么？ xgboost如何调参,xgboost可以自定义损失函数吗？给定一个场景如何自定义损失函数？如果样本的权重不一样如何自定义损失函数？sklearn的xgboost支持哪些损失函数？分类和回归算法都有哪些损失函数？模型融合如何做的？bagging，boosting和stacking的原理以及他们的区别是是什么；

#### 2、LR
损失函数，为什么不用最小二乘  


#### 3、gbdt


#### 4、写出极大似然估计公式


#### 5、kmeans


#### 6、SVM： 拉格朗日乘子，KKT条件，对偶问题，核方法是什么，用过哪些核函数





Naive Bayes, random forest, svm, logistic regression, xgboost, gdbt全家桶，每个都详细问讲区别讲联系讲损失函数
一些模型独有的问题：
- svm核函数 种类，如何选择，如何应用
- random forest引出 bagging boosting，区别
- random forest 分割依据 （忘了）
- random forest 随机在哪里（特征，样本选择）
- lr 和 rf 在输入上的不同
- xgboost 二阶导数
- 等

HMM CRF双雄
- Viterbi

tf-idf 公式 如何用tf-idf计算文本中的关键词




### 一般小问题  

1、写出 LR 和 SVM 损失函数  
2、为什么AUC面积大证明性能强  
3、不平衡样本处理（迪哥课程里有）  
4、



### 已经解决的问题

#### 解决过拟合的方法
增加数据、正则化、Dropout、early stopping、batch norm、  

#### 为什么 L2 可以解决过拟合
权重衰减通过惩罚绝对值较大的参数，为需要学习的模型增加了限制（可以看自己的笔记 d2l 66 页）  

#### 了解什么评估指标
recall、precision、F1、AUC  







