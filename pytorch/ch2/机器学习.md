
### 重要问题

### 期望的公式

#### 1、xgboost 相关
①、  
②、为什么要二阶展开  
③、xgboost采样的时候怎么采样的  
④、手写xgboost的目标函数，xgboost和GBDT的区别，xgboost构建树时候节点分裂的公式是什么？ xgboost如何调参,xgboost可以自定义损失函数吗？给定一个场景如何自定义损失函数？如果样本的权重不一样如何自定义损失函数？sklearn的xgboost支持哪些损失函数？分类和回归算法都有哪些损失函数？模型融合如何做的？bagging，boosting和stacking的原理以及他们的区别是是什么；

#### 2、LR 是问的最多的
损失函数，为什么不用最小二乘  


#### 3、gbdt


#### 4、写出极大似然估计公式


#### 5、kmeans
手写 Kmeans 代码、具体步骤、k 值如何选择、如何初始化、聚的是特征还是样本？特征的距离如何计算？  
什么数据不能聚：圆环套圆圈、笑脸、，非凸数据  



#### 6、SVM： 拉格朗日乘子，KKT条件，对偶问题，核方法是什么，用过哪些核函数


#### 7、特征工程
怎么清洗数据、特征筛选，怎么找出相似性高的特征并去掉、如何比较feature importance、不平衡样本处理（迪哥课程里有）、特征数值范围很大怎么办、  


#### 知道的 loss 都有哪些？
L1、MSE、cross-entropy（讲一讲交叉熵。d2l）、

#### 谈谈对 PCA 的认识  
PCA的流程，PCA第一步为什么要中心化、

#### 介绍 GBDT  



### 一般小问题  

1、写出 LR 和 SVM 损失函数  
2、为什么AUC面积大证明性能强  
3、
4、



### 已经解决的问题

#### 解决过拟合的方法
增加数据、正则化、Dropout、early stopping、batch norm、  

#### 为什么 L2 可以解决过拟合
权重衰减通过惩罚绝对值较大的参数，为需要学习的模型增加了限制（可以看自己的笔记 d2l 66 页）  

#### 了解什么评估指标
交叉验证是模型评估方法、recall、precision、F1、AUC、介绍一下 ROC 曲线（西瓜书）、  

#### DBSCAN
传染病一样，在范围内，发展下线 要指定半径、  
优点：不需要制定簇的个数、可以发现任意形状的簇、  
缺点：参数难以选择，参数对结果影响非常大、训练很慢、


Naive Bayes, random forest, svm, logistic regression, xgboost, gdbt全家桶，每个都详细问讲区别讲联系讲损失函数
一些模型独有的问题：
- svm核函数 种类，如何选择，如何应用
- random forest引出 bagging boosting，区别
- random forest 分割依据 （忘了）
- random forest 随机在哪里（特征，样本选择）
- lr 和 rf 在输入上的不同
- xgboost 二阶导数
- 等

HMM CRF双雄 Viterbi

tf-idf 公式 如何用tf-idf计算文本中的关键词




