
### 重要问题 

#### Batch Norm  

#### RNN 原理，RNN 为什么难以训练，LSTM 又是怎么改进的，LSTM有几个门？各个门的作用是什么？公式是什么？LSTM解决了什么问题

#### 说一说 word embedding、word2vec、skip-gram、




### 一般问题  

1、要 pysnooper 3 个优化器，SGD、动量法、Adam  
d2l 第 7 章  

2、momentum 等优化算法公式，参数含义  



### 已解决问题

#### numpy 实现卷积  
d2l 和鱼书，还是以 d2l 为准  

#### 激活函数
d2l 面试前再读一遍 3.8 节  

#### 1x1卷积核作用
改变通道数  

#### Dropout 的作用
在训练中，隐藏层神经元的丢弃是随机的，输入层的计算无法过度依赖任何一个，从而在训练中起到正则化的作用，可以用来有效应对过拟合。  
Hinton 去银行想打的，如果一直是这些职员，那么就有可能和劫匪或者做假账的串通。经常随机调换，就不会有这个问题。  


#### 梯度消失、梯度爆炸的原因，应该是层数太多了，看书确认一下







